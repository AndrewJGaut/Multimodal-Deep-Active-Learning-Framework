{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.10.0\n",
      "Torchvision Version:  0.11.1\n"
     ]
    }
   ],
   "source": [
    "# Adapted from https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "# for kaggle satellite image classification dataset https://www.kaggle.com/mahmoudreda55/satellite-image-classification\n",
    "# and then basic active learning was applied.\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_finetuning_utils import train_model, train_model_given_numpy_arrays, initialize_model\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# sys.path.insert(0, '..')\n",
    "from test_framework.model_interface import ModelInterface\n",
    "from test_framework.tester import Tester\n",
    "from utils.data_utils import get_kaggle_satellite_image_classification_dataset_as_numpy_arrays\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"../data/kaggle_satellite_image_classification\"\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet]\n",
    "model_name = \"squeezenet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 4\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 1#15\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n",
    "\n",
    "# Flag for whether to perform data augmentation. When True, we perform random\n",
    "#   operations (random crop, random flip, etc) to augment the data. When False,\n",
    "#   we only perform deterministic preprocessing.\n",
    "perform_data_augmentation = False\n",
    "\n",
    "# Number of iterations of the active learning loop to perform\n",
    "num_active_learning_iterations = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data,y_data = get_kaggle_satellite_image_classification_dataset_as_numpy_arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Dropout(p=0.5, inplace=False)\n",
      "  (1): Conv2d(512, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveLearningModel(ModelInterface):\n",
    "    def __init__(self,model,name=\"no name provided\",details=\"no details provided\"):\n",
    "        model.to(device)\n",
    "        self.model = model\n",
    "        self._name = name\n",
    "        self._details = details\n",
    "        # Gather the parameters to be optimized/updated in this run. If we are\n",
    "        #  finetuning we will be updating all parameters. However, if we are\n",
    "        #  doing feature extract method, we will only update the parameters\n",
    "        #  that we have just initialized, i.e. the parameters with requires_grad\n",
    "        #  is True.\n",
    "        params_to_update = model.parameters()\n",
    "        verbose = False\n",
    "        if verbose:\n",
    "            print(\"Params to learn:\")\n",
    "        if feature_extract:\n",
    "            params_to_update = []\n",
    "            for name,param in model.named_parameters():\n",
    "                if param.requires_grad == True:\n",
    "                    params_to_update.append(param)\n",
    "                    if verbose:\n",
    "                        print(\"\\t\",name)\n",
    "        elif verbose:\n",
    "            for name,param in model.named_parameters():\n",
    "                if param.requires_grad == True:\n",
    "                    print(\"\\t\",name)\n",
    "\n",
    "        # Observe that all parameters are being optimized\n",
    "        optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "        self._optimizer = optimizer_ft\n",
    "        \n",
    "        # store criterion\n",
    "        self._criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def name(self) -> str:\n",
    "        return self._name\n",
    "    def details(self) -> str:\n",
    "        return self._details\n",
    "    def train(self, train_x:np.ndarray, train_y:np.ndarray) -> None:\n",
    "        self.model = train_model_given_numpy_arrays(self.model, train_x, train_y, self._criterion, self._optimizer, num_epochs=num_epochs, verbose=False)\n",
    "    def predict(self, test_x, output_tensors=False):\n",
    "        if output_tensors:\n",
    "            return self.model(torch.tensor(test_x).to(device))\n",
    "        else:\n",
    "            return self.model(torch.tensor(test_x).to(device)).cpu().detach().numpy()\n",
    "    def query(self, unlabeled_data:np.ndarray, labeling_batch_size:int) -> np.ndarray:\n",
    "        self.model.eval()\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        softmax_outputs = softmax(self.predict(unlabeled_data,output_tensors=True))\n",
    "        max_softmax_vals, _ = torch.max(softmax_outputs, 1)\n",
    "        max_softmax_vals_array = max_softmax_vals.cpu().detach().numpy()\n",
    "        least_confident_indices = np.argpartition(max_softmax_vals_array,labeling_batch_size)[:labeling_batch_size]\n",
    "        return least_confident_indices\n",
    "    def query2(self, unlabeled_dataloader, labeling_batch_size:int) -> np.ndarray:\n",
    "        self.model.eval()\n",
    "        max_softmax_vals_list = []\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        for inputs, labels in unlabeled_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            softmax_outputs = softmax(self.model(inputs))\n",
    "            max_softmax_vals, _ = torch.max(softmax_outputs, 1)\n",
    "            max_softmax_vals_list += list(max_softmax_vals.cpu().detach().numpy())\n",
    "        max_softmax_vals_array = np.array(max_softmax_vals_list)\n",
    "        least_confident_indices = np.argpartition(max_softmax_vals_array,labeling_batch_size)[:labeling_batch_size]\n",
    "        # print(sorted(max_softmax_vals_array.copy()))\n",
    "        # print(max_softmax_vals_array[least_confident_indices])\n",
    "        return least_confident_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_onehot = np.zeros((y_data.size, y_data.max()+1))\n",
    "y_data_onehot[np.arange(y_data.size),y_data] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = Tester([x_data[:600]],y_data_onehot[:600])\n",
    "tester.TRAINING_EPOCHS = 2\n",
    "tester.ACTIVE_LEARNING_BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_learning_model = ActiveLearningModel(model_ft,model_name,str(model_ft.classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test 0, Training Epoch 1: 100%|██████████| 10/10 [00:14<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "tester.test_model(active_learning_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_data_augmentation:\n",
    "  transform = transforms.Compose([transforms.RandomResizedCrop(224), \n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.RandomHorizontalFlip(),\n",
    "                                  transforms.RandomRotation(20),\n",
    "                                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "else:\n",
    "  transform = transforms.Compose([transforms.CenterCrop(224),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "# train_data, val_data, test_data = torch.utils.data.random_split(dataset, [3950, 1120,561])\n",
    "unlabelled_train_data, labelled_train_data, val_data, test_data = torch.utils.data.random_split(dataset, [4113, 457, 500, 561])\n",
    "print('Num unlabelled train images:', len(unlabelled_train_data))\n",
    "print('Num labelled train images:  ', len(labelled_train_data))\n",
    "print('Num val images:             ', len(val_data))\n",
    "print('Num test images:            ', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloader parameters\n",
    "num_workers=0\n",
    "\n",
    "# prepare data loaders\n",
    "unlabelled_train_loader = torch.utils.data.DataLoader(unlabelled_train_data, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=False)\n",
    "labelled_train_loader = torch.utils.data.DataLoader(labelled_train_data, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=False)\n",
    "valid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, \n",
    "                                          num_workers=num_workers, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "                                          num_workers=num_workers, shuffle=False)\n",
    "dataloaders_dict = {\n",
    "    'unlabelled': unlabelled_train_loader,\n",
    "    'train': labelled_train_loader,\n",
    "    'val': valid_loader,\n",
    "    'test': test_loader\n",
    "}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "# model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# active learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_active_learning_iterations):\n",
    "    active_learning_model.train(dataloaders_dict)\n",
    "    chosen_indices = active_learning_model.query(dataloaders_dict['unlabelled'],5)\n",
    "    print(f\"Iteration {i}: The model wants the following samples labelled: {chosen_indices}\")\n",
    "    negative_chosen_indices = [0] # TODO: make this the complement\n",
    "    dataloaders_dict['unlabelled'] = torch.utils.data.Subset(dataloaders_dict['unlabelled'],negative_chosen_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActiveLearningModel(ModelInterface):\n",
    "    def __init__(self,model,name=\"no name provided\",details=\"no details provided\"):\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.details = details\n",
    "    def name(self) -> str:\n",
    "        return self.name\n",
    "    def details(self) -> str:\n",
    "        return self.details\n",
    "    def train(self, train_x:np.ndarray, train_y:np.ndarray) -> None:\n",
    "        self.model, hist = train_model(self.model, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "    def predict(self, test_x) -> np.ndarray:\n",
    "        return self.model(test_x).cpu().detach().numpy()\n",
    "    def query(self, unlabeled_dataloader, labeling_batch_size:int) -> np.ndarray:\n",
    "        self.model.eval()\n",
    "        max_softmax_vals_list = []\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        for inputs, labels in unlabeled_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            softmax_outputs = softmax(self.model(inputs))\n",
    "            max_softmax_vals, _ = torch.max(softmax_outputs, 1)\n",
    "            max_softmax_vals_list += list(max_softmax_vals.cpu().detach().numpy())\n",
    "        max_softmax_vals_array = np.array(max_softmax_vals_list)\n",
    "        least_confident_indices = np.argpartition(max_softmax_vals_array,labeling_batch_size)[:labeling_batch_size]\n",
    "        # print(sorted(max_softmax_vals_array.copy()))\n",
    "        # print(max_softmax_vals_array[least_confident_indices])\n",
    "        return least_confident_indices"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "572989d3e5e970065861bb6fc65bf74562091858dbe06814b9896f38cf561588"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('cs229proj': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
